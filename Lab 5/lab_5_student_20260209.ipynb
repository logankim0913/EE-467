{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki9MpgEX6mbi"
      },
      "source": [
        "# EE 467 Lab 5: Detecting Credit Card Fraud with Decision Tree\n",
        "\n",
        "In lab 5 and lab 6, we will go through a famous machine learning contest on Kaggle-**detecting fraud credit card transactions**, and learn about **decision tree, random forest and ensemble learning algorithms** in the meantime. Today, our task is to analyze and pre-process the credit card transactions dataset to resolve its inherent issues. We will then train a decision tree classifier on the transform features to identify fraud credit card transactions.\n",
        "\n",
        "## Task Description\n",
        "### Background\n",
        "\n",
        "Fraudulent credit card transactions create major losses for both cardholders and financial institutions. According to the Nilson Report, worldwide payment card fraud losses were \\$33.83B in 2023 and \\$33.41B in 2024.\n",
        "\n",
        "### Challenges in Detection\n",
        "\n",
        "1. Uncommon: Fraud transactions are rare compared to normal transactions, hence there are few samples where we can learn from to identify these transactions. We will see in the later part of this lab that less than 0.2% of the transactions are in the fraud class.\n",
        "2. Concealed: Fraudsters will try their best to blend in and conceal their activities.\n",
        "3. Change over time: Patterns of fraud transactions change over time so that fraudsters could avoid getting caught.\n",
        "\n",
        "### Evaluate Machine Learning-based Fraud Detection\n",
        "\n",
        "Traditionally, rules-based expert systems are deployed to catch fraud transactions. While they could do an excellent jon to uncover existing fraud patterns, they are not effective for new and covert fraud schemes. Recently, financial institutions start to apply machine learning algorithms to credit card fraud identification because of their flexibility and adaptation to new fraud patterns. However, when compared to regular machine learning tasks, there are a few extra requirements for evaluation of the fraud transaction classifier:\n",
        "\n",
        "* Instead of looking at the overall accuracy, we are more interested in the precision and recall, as the credit card transactions dataset is highly unbalanced. A deterministic classifier that always marks transactions as non-fraud could achieve over 99.8% of overall accuracy, but such a classifier is meaningless in terms of identifying fraud because its precision and recall in the fraud class are both zero.\n",
        "* Besides, we also want to reduce false positives because in that case normal transactions are also blocked, which can be annoying for benign credit card users.\n",
        "\n",
        "## Lab Dependencies\n",
        "\n",
        "Apart from `numpy`, `pandas`, `scikit-learn` (`sklearn`) and `matplotlib`, we need to install two more external libraries for this lab: `seaborn` and `imbalanced-learn` (`imblearn`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbuXNpGm6mbk"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas scikit-learn matplotlib seaborn imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xJf credit-card.tar.xz #<--- Use this to unzip the dataset folder in Colab"
      ],
      "metadata": {
        "id": "b3tqS8If7jT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQGwiOxn6mbk"
      },
      "source": [
        "## Feature Analysis\n",
        "\n",
        "Before building machine learning classification models, we need to know more about the characteristics of the credit card transactions dataset. As usual, we will begin with loading the credit card dataset into memory and preview its format and statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsRK7inf6mbk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the credit card dataset into memory\n",
        "df = pd.read_csv(\"creditcard.csv\")\n",
        "\n",
        "# Preview the first few records\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVDUUG5D6mbl"
      },
      "outputs": [],
      "source": [
        "# Preview the statistics of each column\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmvYp-gi6mbl"
      },
      "source": [
        "The preview results indicate that there are 30 features for each credit card transaction (record), of which 28 of them (`V1` to `V28`) are **opaque features obtained from Principle Component Analysis (PCA)**. The `Time` and `Amount` features indicate the **relative time and amount** of each transaction and none of them is normalized (re-scaled). The `Class` column serves as the **labels of all transactions**, and we are interested in the categories and distributions of labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdjrSqfd6mbl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of transaction classes (fraud / non-fraud)\n",
        "cat_dist = df[\"Class\"].value_counts()\n",
        "print(\"Frequencies of classes:\\n\", cat_dist, \"\\n\")\n",
        "\n",
        "# Ratio of classes\n",
        "print(\"Ratio of classes:\\n\", cat_dist/len(df))\n",
        "\n",
        "# Plot the distribution of classes\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"# of Transactions\")\n",
        "cat_dist.plot(kind=\"bar\", title=\"Distribution of classes\", rot=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxudheNU6mbl"
      },
      "source": [
        "Apparently, the dataset is **highly imbalanced** in terms of class distributions, as it contains only **492 fraud cases** out of **284,807 total cases**, or **0.172% of fraud cases**. This can be justified by the low frequent and covert nature of fraudulent transactions. In the later part of this lab we will see that special measures must be used to deal with this data imbalance issue.\n",
        "\n",
        "Next, we will look into each kind of feature individually and try to reveal patterns in credit card transactions.\n",
        "\n",
        "### Opaque Features (`V1` to `V28`)\n",
        "\n",
        "The opaque features (`V1` to `V28`) are obtained by reducing the dimensions of raw features with PCA. The raw transaction features of credit card transactions are not provided because of privacy issues, and we have no idea about the meaning of these opaque features. In this case, one approach is to **perform PCA again on the opaque features**, reducing them to three dimensions and then **visualizing them on 2D and 3D plots** (Mathmatically this is equivalent to directly reducing raw features to three dimensions with a single PCA transformation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gbWLPff6mbl"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "import lab_6_util\n",
        "\n",
        "with lab_6_util.timeit(\"Visualizing opaque features to 3D\"):\n",
        "    # Reduce opaque features to three dimensions\n",
        "    opaque_features_3d = IncrementalPCA(n_components=3).fit_transform(df.loc[:, \"V1\":\"V28\"])\n",
        "    # Visualize three-dimensional opaque features\n",
        "    lab_6_util.visualize_samples(opaque_features_3d, df[\"Class\"], \"3D opaque features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlET4v-k6mbl"
      },
      "source": [
        "The visualization plot shows that a significant portion of fraud transactions can be separated from normal transactions in the 3D space. However, to achieve better classification performance, we should incorporate additional features beyond just the opaque PCA features.\n",
        "\n",
        "### Transaction Time\n",
        "\n",
        "To reveal potential patterns in transaction time, we plot its distribution with `seaborn`, a library for statistical data visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPNyayeG6mbl"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Convert transaction time from seconds to hours\n",
        "time_df = df[\"Time\"] / (60 * 60)\n",
        "\n",
        "# Plot the distribution of transaction time\n",
        "sns.histplot(time_df, color=\"r\", kde=True, stat=\"density\")\n",
        "# Set plot title\n",
        "plt.title(\"Distribution of transaction time\")\n",
        "# Set plot range\n",
        "plt.xlim((time_df.min(), time_df.max()))\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rnzXdoa6mbl"
      },
      "source": [
        "The plot indicates that all transactions in the dataset were collected during a **two-day period** (maximum time is 48 hours). Most transactions occur during a continuous 16-hour period in a single day, corresponding to when people are typically awake. Note that time 0 is a relative timestamp and does not correspond to any specific clock time.\n",
        "\n",
        "Now let's examine the distribution **separately for fraud and non-fraud transactions** to identify any temporal patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kUPGVYD46mbl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Fraud and non-fraud transactions\n",
        "df_nonfraud = df[df[\"Class\"]==0]\n",
        "df_fraud = df[df[\"Class\"]==1]\n",
        "\n",
        "# X-axis of the transaction time plot\n",
        "bins = np.arange(48)\n",
        "# Transaction time for fraud and non-fraud transactions (in hours)\n",
        "time_nonfraud = df_nonfraud[\"Time\"]/(60*60)\n",
        "time_fraud = df_fraud[\"Time\"]/(60*60)\n",
        "\n",
        "## [ TODO ]\n",
        "# Plot transaction time distribution of fraud and non-fraud transactions SEPARATELY\n",
        "# (Caution: plot the percentiles (NOT amount) for each range)\n",
        "# (Hint: use `plt.hist` to plot; set fraud and non-fraud series to differnent colors and increase transparency)\n",
        "raise NotImplementedError\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seW_ANsw6mbm"
      },
      "source": [
        "We can observe from the plot that the proportion of non-fraud transactions decreases from hour 1 to 8 and from hour 24 to 32, while the proportion of fraud transactions increases during these periods. This suggests that **fraud transactions tend to occur more frequently during night hours**.\n",
        "\n",
        "### Transaction Amount\n",
        "\n",
        "Similar to the transaction time, let's also plot the distribution of transaction amount:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eIlgmG96mbm"
      },
      "outputs": [],
      "source": [
        "# Amount of all transactions\n",
        "amount_df = df[\"Amount\"]\n",
        "\n",
        "# Plot the distribution of transaction amount\n",
        "sns.histplot(amount_df, color=\"r\", kde=True, stat=\"density\")\n",
        "# Set plot title\n",
        "plt.title(\"Distribution of transaction amount\")\n",
        "# Set plot range\n",
        "plt.xlim((0, amount_df.max()))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnW4nKk86mbm"
      },
      "source": [
        "The distribution of transaction amount exhibits a classic **long-tail distribution**: most transactions are of low value, but there are outliers with amounts several times the mean. This characteristic can be seen in the statistics of non-fraud, fraud, and all transactions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTwnCb1a6mbm"
      },
      "outputs": [],
      "source": [
        "# Distribution percentiles\n",
        "dist_pc = np.arange(0.1, 1, 0.1)\n",
        "\n",
        "# Amount of non-fraud and fraud transactions\n",
        "amount_nonfraud = df_nonfraud[\"Amount\"]\n",
        "amount_fraud = df_fraud[\"Amount\"]\n",
        "\n",
        "# Statistics of amount of non-fraud, fraud and all transactions\n",
        "amount_stats = pd.DataFrame.from_dict({\n",
        "    \"Non-Fraud\": amount_nonfraud.describe(percentiles=dist_pc),\n",
        "    \"Fraud\": amount_fraud.describe(percentiles=dist_pc),\n",
        "    \"All\": amount_df.describe(percentiles=dist_pc)\n",
        "})\n",
        "\n",
        "amount_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfna9Iv-6mbm"
      },
      "source": [
        "The average fraud transaction amount is $122 USD, while non-fraud transactions average $88 USD. The distributions are similar for the bottom 60% of transactions, but **fraud transactions have higher values in the top 40%**. We can further demonstrate this pattern by plotting the distribution of high-value transactions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpcJNbuD6mbm"
      },
      "outputs": [],
      "source": [
        "# X-axis of the transaction amount plot\n",
        "bins = np.arange(200, 2500, 25)\n",
        "\n",
        "# Plot high-value fraud and non-fraud transactions ($200 to $2000)\n",
        "plt.hist(amount_nonfraud, bins, density=True, label=\"Non-Fraud\", color=\"b\", alpha=0.5)\n",
        "plt.hist(amount_fraud, bins, density=True, label=\"Fraud\", color=\"r\", alpha=0.5)\n",
        "# Set plot title\n",
        "plt.title(\"Distribution of amount of high-value transactions\")\n",
        "# Show legend\n",
        "plt.legend(fancybox=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUvrDONc6mbm"
      },
      "source": [
        "We can conclude from this plot that fraud transactions tend to occur more frequently in the high-value \"tail\" of the transaction distribution. However, the amount feature alone would still be insufficient for accurate fraud detection.\n",
        "\n",
        "## Feature Scaling\n",
        "\n",
        "With the analysis of different feature types complete, we now move to the **feature scaling** step. The opaque features (`V1` to `V28`) are generated by PCA and are already normalized (measured in units of standard deviations), so they require no additional preprocessing. However, the Transaction Time and Amount features are raw values that need rescaling:\n",
        "\n",
        "- **`StandardScaler`** could be applied to transaction time (after taking modulo 24 to get hour-of-day), but it is inappropriate for transaction amount because the amount distribution is not normal.\n",
        "- **`MinMaxScaler`** is also inappropriate for transaction amount because the long-tail distribution contains many outliers that would distort the scaling.\n",
        "\n",
        "Instead, we use **[`RobustScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)**, which performs a linear transformation using the interquartile range (IQR) as the scale factor, making it robust against outliers:\n",
        "\n",
        "$$x_{\\text{scaled}} = \\frac{x - Q_{25}(x)}{Q_{75}(x) - Q_{25}(x)}$$\n",
        "\n",
        "This formula uses the 25th percentile ($Q_{25}$) as the baseline and the difference between 75th and 25th percentiles as the scale, ensuring that outliers have minimal impact on the transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FMjcIUe6mbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "# Copy original dataset\n",
        "df_scaled = df.copy()\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Divide transaction time by 24 (hours in a day)\n",
        "# 2) Rescale and override transaction time column with `StandardScaler`\n",
        "# 3) Rescale and override transaction amount column with `RobustScaler`\n",
        "#    (Hint: refer to lab 5 and API reference for usage of feature scalers)\n",
        "raise NotImplementedError\n",
        "\n",
        "# Review scaling results\n",
        "df_scaled[[\"Amount\", \"Time\"]].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHjjdUxs6mbm"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "We now begin the training stage by performing a **train-test split** on the scaled dataset. Since fraud transactions are rare, we set the **test set size to 40%** to ensure sufficient fraud samples are included in the test set for reliable evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIv-dpwm6mbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get feature and label values from original dataset\n",
        "feat_all = df_scaled.drop([\"Class\"], axis=1).values\n",
        "y_all = df_scaled[\"Class\"].values\n",
        "\n",
        "# Split samples into training and test sets\n",
        "feat_train, feat_test, y_train, y_test = train_test_split(\n",
        "    feat_all, y_all, test_size=0.4, random_state=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2pVVau6mbm"
      },
      "source": [
        "### Data Resampling with SMOTE\n",
        "\n",
        "Our earlier analysis showed that the credit card fraud dataset is highly imbalanced: over 99.8% of transactions are non-fraud. To address this, we apply **data resampling** to adjust the class distribution before training. We can either **over-sample** the fraud class or **under-sample** the non-fraud class; here we use **Synthetic Minority Over-sampling TEchnique (SMOTE)**.\n",
        "\n",
        "SMOTE synthesizes new minority samples through $k$-**nearest neighbors**:\n",
        "\n",
        "1. Randomly select a fraud sample and find its $k$ nearest neighbors.\n",
        "2. Randomly select one of these neighbors.\n",
        "3. Create a synthetic sample via linear interpolation between the two selected samples.\n",
        "\n",
        "An extension of SMOTE, called **borderline SMOTE**, enhances the original technique by first learning an **decision boundary** between the majority and minority classes and then applying SMOTE only to minor samples that are adjacent to the decision boundary. Here we will try both original and borderline SMOTE for the fraud class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FTOMmPAd6mbm"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
        "\n",
        "# SMOTE over-sampling\n",
        "smote = SMOTE(random_state=0)\n",
        "# Borderline SMOTE over-sampling\n",
        "borderline_smote = BorderlineSMOTE(random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-XAomoi6mbm"
      },
      "source": [
        "### Decision Tree Classifier\n",
        "\n",
        "A **decision tree** is a machine learning model that uses a hierarchical, tree-like structure for making predictions. To make a prediction with a decision tree:\n",
        "\n",
        "1. Start at the root node and evaluate the associated condition\n",
        "2. Follow the appropriate branch based on the condition result\n",
        "3. Recursively evaluate conditions at each sub-node, moving deeper into the tree\n",
        "4. Continue until reaching a leaf node, which provides the final prediction\n",
        "\n",
        "Decision trees are versatile and work for both classification and regression problems. Below are visualizations showing a simple decision tree and its decision boundary:\n",
        "\n",
        "\n",
        "Now, we will build and train three [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). The first one will be trained directly on the original samples, while the second and third one will be trained with the fraud class over-sampled using original and borderline SMOTE respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-rh5KCZ6mbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Train a decision tree classifier `dt` on original samples.\n",
        "# 2) Build a ML pipeline `dt_smote` with SMOTE over-sampling and a decision tree classifier,\n",
        "#    then train the whole pipeline.\n",
        "# 3) Build a ML pipeline `dt_borderline_smote` with borderline SMOTE over-sampling and\n",
        "#    a decision tree classifier, then train the whole pipeline\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgwNyah86mbm"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "With the three decision tree classifiers trained, we now evaluate their performance on the test set using the **confusion matrix** and **classification report**, which includes the metrics we encountered before:\n",
        "\n",
        "- **Precision**: Of the transactions flagged as fraud, how many actually were fraud?\n",
        "- **Recall**: Of all fraud transactions in the test set, how many did we correctly identify?\n",
        "- **F1-score**: The harmonic mean of precision and recall, balancing both metrics\n",
        "- **Accuracy**: Overall correctness, though less useful for imbalanced datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNUHAwg56mbm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def plot_m2m_curve(ax, xs, ys, x_metric, y_metric, title):\n",
        "    \"\"\"Helper function for plotting metric-to-metric curve.\"\"\"\n",
        "    # Plot metric-to-metric curve\n",
        "    ax.plot(xs, ys)\n",
        "\n",
        "    # Make sub-plot square\n",
        "    ax.set_aspect(\"equal\")\n",
        "    # Set axes labels\n",
        "    ax.set_xlabel(x_metric)\n",
        "    ax.set_ylabel(y_metric)\n",
        "    # Set title\n",
        "    ax.set_title(title)\n",
        "\n",
        "def evaluate_model(model, name, feat_test, y_test):\n",
        "    \"\"\" Evaluate a classification model on the test set, then print and plot metrics. \"\"\"\n",
        "    # Make prediction from features\n",
        "    pred_test = model.predict(feat_test)\n",
        "\n",
        "    print(f\"\\n[ Evaluation result for {name} ]\")\n",
        "    # Print classification report\n",
        "    print(\"Classification report:\")\n",
        "    print(classification_report(y_test, pred_test))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_test, pred_test), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi_EBgrc6mbm"
      },
      "outputs": [],
      "source": [
        "# Evaluate all three trained models\n",
        "evaluate_model(dt, \"DT (no resampling)\", feat_test, y_test)\n",
        "evaluate_model(dt_smote, \"DT with SMOTE\", feat_test, y_test)\n",
        "evaluate_model(dt_borderline_smote, \"DT with borderline SMOTE\", feat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsPWlvqS6mbm"
      },
      "source": [
        "From the evaluation results, we observe that **over-sampling the fraud class does not improve precision, recall, or F1-score**. In fact, performance degradation is observed when original SMOTE over-sampling is applied. This suggests that simple decision trees may be limited for this task. In the next lab, we will explore ensemble learning methods and random forests to improve classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57vPjhNf6mbm"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Kaggle credit card fraud dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
        "2. Seaborn API reference: https://seaborn.pydata.org/api.html\n",
        "3. SMOTE over-sampling: https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis#SMOTE\n",
        "4. SMOTE for imbalance data classification: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
        "5. `imbalanced-learn` over-sampling API reference: https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.over_sampling\n",
        "6. Receiver Operating Characteristic (ROC) curve: https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
        "7. ROC Curves and Precision-Recall Curves for Imbalanced Classification: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n",
        "8. Decision tree: https://en.wikipedia.org/wiki/Decision_tree\n",
        "9. Decision tree learning: https://en.wikipedia.org/wiki/Decision_tree_learning\n",
        "10. Decision tree algorithm with example: https://www.youtube.com/watch?v=RmajweUFKvM"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}